{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoConfig, AudioClassificationPipeline\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTalTechNLP/voxlingua107-epaca-tdnn - 0.66\\nspeechbrain/google_speech_command_xvector - 0.90\\nanton-l/distilhubert-ft-common-language - 0\\nanton-l/sew-d-mid-400k-ft-keyword-spotting - >300 MB\\n\\n\\nw11wo/distil-wav2vec2-adult-child-cls-v3 - 199MB \\nanantoj/distil-wav2vec2-adult-child-cls - 144MB\\nntu-spml/distilhubert - 89MB\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TalTechNLP/voxlingua107-epaca-tdnn - 0.66\n",
    "speechbrain/google_speech_command_xvector - 0.90\n",
    "anton-l/distilhubert-ft-common-language - 0\n",
    "anton-l/sew-d-mid-400k-ft-keyword-spotting - >300 MB\n",
    "\n",
    "pretrained_models/distil-wav2vec2-finetuned-ks/checkpoint-15960\n",
    "\n",
    "w11wo/distil-wav2vec2-adult-child-cls-v3 - 199MB \n",
    "anantoj/distil-wav2vec2-adult-child-cls - 144MB\n",
    "ntu-spml/distilhubert - 89MB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "WEIGHTS_PATH = Path(\"pretrained_models/xvector_finetuned\")\n",
    "EXP_NAME = WEIGHTS_PATH.name\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "CLASSES = [\n",
    "    \"down\",\n",
    "    \"go\",\n",
    "    \"left\",\n",
    "    \"no\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"right\",\n",
    "    \"stop\",\n",
    "    \"up\",\n",
    "    \"yes\",\n",
    "]\n",
    "label2id = dict([[v, k] for k, v in enumerate(CLASSES)])\n",
    "id2label = dict([[k, v] for k, v in enumerate(CLASSES)])\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "N_NEIGHBORS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_classifier = EncoderClassifier.from_hparams(\n",
    "    source=WEIGHTS_PATH,\n",
    "    savedir=Path(\"pretrained_models\") / EXP_NAME,\n",
    "    run_opts={\"device\": DEVICE},\n",
    ")\n",
    "audio_normalizer = enc_classifier.audio_normalizer\n",
    "label_encoder = enc_classifier.hparams.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    signal, sr = torchaudio.load(str(path), channels_first=False)\n",
    "    return audio_normalizer(signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='cosine', n_jobs=-1, n_neighbors=20)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(\n",
    "    n_neighbors=N_NEIGHBORS,\n",
    "    metric=\"cosine\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbcc3b4028e4b42a58954fb027bfb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_filepaths = sorted(list((DATA_PATH / \"train\").rglob(\"*.wav\")))\n",
    "rel_length = torch.tensor([1.0])\n",
    "lengths = []\n",
    "classes = []\n",
    "embeddings = []\n",
    "for audiofile in tqdm(audio_filepaths):\n",
    "    class_name = audiofile.parts[-2]\n",
    "\n",
    "    wav = load_audio(audiofile).unsqueeze(0)\n",
    "    embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0).squeeze(0)\n",
    "\n",
    "    classes.append(class_name)\n",
    "    embeddings.append(embedding.cpu().numpy())\n",
    "    lengths.append(wav.shape[-1] / 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfcfae760af4ff5b223be7f47bcde23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "pred = []\n",
    "probas = []\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    wav = load_audio(audiofile).unsqueeze(0)\n",
    "    output = enc_classifier.classify_batch(wav, rel_length)\n",
    "    class_name = output[-1][-1]\n",
    "    out_probs = output[0]\n",
    "    probas.append(out_probs)\n",
    "    pred.append(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2ForSequenceClassification(\n",
      "  (wav2vec2): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2GroupNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2Encoder(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(WEIGHTS_PATH)\n",
    "# print(config)\n",
    "enc_model = AutoModelForAudioClassification.from_pretrained(WEIGHTS_PATH)\n",
    "# enc_model.save_pretrained(Path(\"pretrained_models\") / EXP_NAME)\n",
    "enc_model.classifier = torch.nn.Linear(256, 256)\n",
    "enc_model = enc_model.to(DEVICE)\n",
    "print(enc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d3a335a460482092e5b3b2105137a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_filepaths = sorted(list((DATA_PATH / \"train\").rglob(\"*.wav\")))\n",
    "\n",
    "classes = []\n",
    "embeddings = []\n",
    "for audiofile in tqdm(audio_filepaths):\n",
    "    class_name = audiofile.parts[-2]\n",
    "\n",
    "    wav, sr = torchaudio.load(audiofile)\n",
    "    wav = wav.to(DEVICE)\n",
    "    embedding = enc_model(wav).logits[0]\n",
    "\n",
    "    classes.append(class_name)\n",
    "    embeddings.append(embedding.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"filepath\": [str(ap) for ap in audio_filepaths],\n",
    "        \"category\": [CLASSES[name] for name in classes],\n",
    "        \"embedding\": embeddings \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>category</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/train/down/0.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[15.127331, 11.903127, 3.8200674, -0.08791563,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/train/down/1.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[14.426889, 15.1516075, 3.523267, 2.2925537, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/train/down/10.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[17.777142, 2.83203, 0.29956368, 9.412637, 12....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/train/down/100.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[12.743387, 12.060911, 0.9379118, 7.697887, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/train/down/1000.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[14.446474, 8.608712, 8.189811, 10.2905855, 8....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filepath  category  \\\n",
       "0     ../data/train/down/0.wav         3   \n",
       "1     ../data/train/down/1.wav         3   \n",
       "2    ../data/train/down/10.wav         3   \n",
       "3   ../data/train/down/100.wav         3   \n",
       "4  ../data/train/down/1000.wav         3   \n",
       "\n",
       "                                           embedding  \n",
       "0  [15.127331, 11.903127, 3.8200674, -0.08791563,...  \n",
       "1  [14.426889, 15.1516075, 3.523267, 2.2925537, 9...  \n",
       "2  [17.777142, 2.83203, 0.29956368, 9.412637, 12....  \n",
       "3  [12.743387, 12.060911, 0.9379118, 7.697887, 14...  \n",
       "4  [14.446474, 8.608712, 8.189811, 10.2905855, 8....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88790, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"../data/train_xvector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125490/59594927.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_embeddings = torch.tensor(embeddings, device=DEVICE)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3d51a365ee4ec49cbc7baf3eee0ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeddings = torch.tensor(embeddings, device=DEVICE)\n",
    "\n",
    "pred = []\n",
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    # wav = load_audio(audiofile).unsqueeze(0)\n",
    "    # embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0)\n",
    "\n",
    "    wav, sr = torchaudio.load(audiofile)\n",
    "    wav = wav.to(DEVICE)\n",
    "    embedding = enc_model(wav).logits\n",
    "    \n",
    "    similarities = cos_sim(embedding, train_embeddings)\n",
    "    max_similar_idx = similarities.argmax()\n",
    "    class_name = classes[max_similar_idx]\n",
    "\n",
    "    pred.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "down    29620\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "down     3208\n",
       "stop     2967\n",
       "left     2965\n",
       "on       2954\n",
       "up       2950\n",
       "off      2932\n",
       "yes      2932\n",
       "no       2924\n",
       "right    2916\n",
       "go       2872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data={\n",
    "    \"id\": [a.stem for a in test_audio_filepaths],\n",
    "    \"category\": pred,\n",
    "})\n",
    "sub.to_csv(f\"submission_{EXP_NAME}_cos_sim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN\n",
    "classifier.fit(embeddings, [label2id[name] for name in classes])\n",
    "# save knn model\n",
    "with open(\"knn_xvector.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(classifier, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4a03cf509742589b88efc0ab379441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "test_embeddings = []\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    wav = load_audio(audiofile).unsqueeze(0)\n",
    "    embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0).squeeze(0)\n",
    "    test_embeddings.append(embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512,), (256,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings[0].shape, embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_efficientnet(weights_path=None):\n",
    "    # === Efficientnet ===\n",
    "    model = torch.hub.load(\n",
    "        \"NVIDIA/DeepLearningExamples:torchhub\",\n",
    "        \"nvidia_efficientnet_b0\",\n",
    "        pretrained=True,\n",
    "    )\n",
    "    model.stem.conv = nn.Conv2d(\n",
    "        1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
    "    )\n",
    "    num_ftrs = model.classifier.fc.in_features\n",
    "    model.classifier.fc = nn.Linear(num_ftrs, 10, bias=True)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    return model\n",
    "\n",
    "MAX_AUDIO_LEN = 16000\n",
    "class MelCreator:\n",
    "    def __init__(self) -> None:\n",
    "        self.make_melspec = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=128,\n",
    "            f_min=55.0,\n",
    "            f_max=7600,\n",
    "            pad=0,\n",
    "            n_mels=128,\n",
    "            window_fn=torch.hann_window,\n",
    "            power=2.0,\n",
    "            normalized=True,\n",
    "            center=False,\n",
    "            pad_mode=\"reflect\",\n",
    "            onesided=True,\n",
    "            norm=\"slaney\",  #'slaney',\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        self.atdb = T.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "    def __call__(self, audio):\n",
    "        melspec = self.atdb(self.make_melspec(audio))\n",
    "        return melspec\n",
    "\n",
    "\n",
    "def __pad_audio(audio):\n",
    "    if MAX_AUDIO_LEN - audio.shape[-1] > 0:\n",
    "        i = np.random.randint(0, MAX_AUDIO_LEN - audio.shape[-1])\n",
    "    else:\n",
    "        i = 0\n",
    "    pad_patern = (i, MAX_AUDIO_LEN - audio.shape[-1] - i)\n",
    "    audio = F.pad(audio, pad_patern, \"constant\").detach()\n",
    "\n",
    "    return audio\n",
    "\n",
    "mel_creator = MelCreator()\n",
    "\n",
    "def preprocess(audio_path):\n",
    "    audio, _ = torchaudio.load(audio_path)\n",
    "    audio = __pad_audio(audio)\n",
    "    audio = audio / audio.abs().max()\n",
    "    melspec = mel_creator(audio)\n",
    "    return melspec\n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, audio_dir: Path, markup_path) -> None:\n",
    "        super().__init__()\n",
    "        self.audio_len = MAX_AUDIO_LEN\n",
    "        self.mel_creator = MelCreator()\n",
    "\n",
    "        self.audio_paths = list()\n",
    "        self.classes = list()\n",
    "        markup = pd.read_csv(markup_path)\n",
    "        for file_name, category in markup.values:\n",
    "            audio, _ = torchaudio.load(audio_dir / f\"{file_name}.wav\")\n",
    "            self.audio_paths.append(audio)\n",
    "            self.classes.append(category)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.audio_paths[idx]\n",
    "        audio = self.__pad_audio(audio)\n",
    "        audio = audio / audio.abs().max()\n",
    "        melspec = self.mel_creator(audio)\n",
    "\n",
    "        return melspec, CLASSES.index(self.classes[idx])\n",
    "\n",
    "    def __pad_audio(self, audio):\n",
    "        if self.audio_len - audio.shape[-1] > 0:\n",
    "            i = np.random.randint(0, self.audio_len - audio.shape[-1])\n",
    "        else:\n",
    "            i = 0\n",
    "        pad_patern = (i, self.audio_len - audio.shape[-1] - i)\n",
    "        audio = F.pad(audio, pad_patern, \"constant\").detach()\n",
    "\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/and/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# resnet = torch.jit.load()\n",
    "eff_net = load_efficientnet(\"pretrained_models/efficientnet_40ep.pt\")\n",
    "eff_net = eff_net.to(DEVICE)\n",
    "xvector = enc_classifier\n",
    "\n",
    "# weights = [2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb9be70741349e39cdd3007ef142357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb Cell 31'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000033vscode-remote?line=5'>6</a>\u001b[0m wav \u001b[39m=\u001b[39m load_audio(audiofile)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000033vscode-remote?line=6'>7</a>\u001b[0m embedding \u001b[39m=\u001b[39m enc_classifier\u001b[39m.\u001b[39mencode_batch(wav, rel_length)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000033vscode-remote?line=7'>8</a>\u001b[0m knn_proba \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict_proba([embedding\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()])[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000033vscode-remote?line=8'>9</a>\u001b[0m knn_probas\u001b[39m.\u001b[39mappend(knn_proba)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:256\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=239'>240</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=240'>241</a>\u001b[0m     \u001b[39m\"\"\"Return probability estimates for the test data X.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=241'>242</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=242'>243</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=253'>254</a>\u001b[0m \u001b[39m        by lexicographic order.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=254'>255</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=255'>256</a>\u001b[0m     neigh_dist, neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X)\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=257'>258</a>\u001b[0m     classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py?line=258'>259</a>\u001b[0m     _y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py:752\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=748'>749</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=749'>750</a>\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=751'>752</a>\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=752'>753</a>\u001b[0m         pairwise_distances_chunked(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=753'>754</a>\u001b[0m             X,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=754'>755</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=755'>756</a>\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=756'>757</a>\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=757'>758</a>\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=758'>759</a>\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=759'>760</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=760'>761</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=762'>763</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py?line=763'>764</a>\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1717\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1714'>1715</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1715'>1716</a>\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1716'>1717</a>\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1717'>1718</a>\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1718'>1719</a>\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1719'>1720</a>\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1720'>1721</a>\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1721'>1722</a>\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1722'>1723</a>\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1889\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1885'>1886</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1886'>1887</a>\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1888'>1889</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1435\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1432'>1433</a>\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1433'>1434</a>\u001b[0m ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1434'>1435</a>\u001b[0m Parallel(backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreading\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49mn_jobs)(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1435'>1436</a>\u001b[0m     fd(func, ret, s, X, Y[s], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1436'>1437</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs))\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1437'>1438</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1439'>1440</a>\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1440'>1441</a>\u001b[0m     \u001b[39m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1441'>1442</a>\u001b[0m     \u001b[39m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1442'>1443</a>\u001b[0m     np\u001b[39m.\u001b[39mfill_diagonal(ret, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=1052'>1053</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=1054'>1055</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=1055'>1056</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=1056'>1057</a>\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=1057'>1058</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=932'>933</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=933'>934</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=934'>935</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=935'>936</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/joblib/parallel.py?line=936'>937</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=763'>764</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=764'>765</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=765'>766</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=766'>767</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=760'>761</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=761'>762</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=555'>556</a>\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=556'>557</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/threading.py?line=557'>558</a>\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=558'>559</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=299'>300</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=300'>301</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/threading.py?line=301'>302</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=302'>303</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=303'>304</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "knn_probas = []\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    # with torch.no_grad():\n",
    "        # xvector \n",
    "        wav = load_audio(audiofile).unsqueeze(0)\n",
    "        embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0).squeeze(0)\n",
    "        knn_proba = classifier.predict_proba([embedding.cpu().numpy()])[0]\n",
    "        knn_probas.append(knn_proba)\n",
    "        # # efficientnet\n",
    "        # melspec = preprocess(audiofile).to(DEVICE)\n",
    "        # eff_logits = eff_net(melspec.unsqueeze(0))\n",
    "        # eff_probas = torch.softmax(eff_logits, dim=1)[0].cpu()\n",
    "\n",
    "        # avg_probas = np.average(\n",
    "        #     np.stack([knn_probas, eff_probas]), \n",
    "        #     axis=0,\n",
    "        #     weights=[1, 2]\n",
    "        # )\n",
    "\n",
    "        # res = id2label[avg_probas.argmax()]\n",
    "        # pred.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = classifier.predict_proba(test_embeddings)\n",
    "pred = [id2label[np.argmax(p)] for p in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (numpy.ndarray, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb Cell 35'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000043vscode-remote?line=0'>1</a>\u001b[0m probas \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39msoftmax(p, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m probas]\n",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb Cell 35'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000043vscode-remote?line=0'>1</a>\u001b[0m probas \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49msoftmax(p, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m probas]\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (numpy.ndarray, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "probas = [torch.softmax(p, dim=1)[0].tolist() for p in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data={\n",
    "    \"path\": [str(a) for a in test_audio_filepaths],\n",
    "    \"category\": pred,\n",
    "    \"proba0\": [p[0] for p in probas],\n",
    "    \"proba1\": [p[1] for p in probas],\n",
    "    \"proba2\": [p[2] for p in probas],\n",
    "    \"proba3\": [p[3] for p in probas],\n",
    "    \"proba4\": [p[4] for p in probas],\n",
    "    \"proba5\": [p[5] for p in probas],\n",
    "    \"proba6\": [p[6] for p in probas],\n",
    "    \"proba7\": [p[7] for p in probas],\n",
    "    \"proba8\": [p[8] for p in probas],\n",
    "    \"proba9\": [p[9] for p in probas],\n",
    "})\n",
    "sub.to_csv(f\"probas_test_{EXP_NAME}-eer-0.025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data={\n",
    "    \"id\": [a.stem for a in test_audio_filepaths],\n",
    "    \"category\": pred,\n",
    "})\n",
    "sub.to_csv(f\"submission_{EXP_NAME}-eer-0.025-knn20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from efficientnet_train import CLASSES, DEVICE\n",
    "from efficientnet_train import TrainData as EfficientnetTrainData\n",
    "from efficientnet_train import TestData\n",
    "from efficientnet_train import load_model as load_efficientnet_model\n",
    "from resnet_train import TrainData as ResnetTrainData\n",
    "from resnet_train import load_model as load_resnet_model\n",
    "\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "def resnet_train_infer(model):\n",
    "    noise_dir = DATA_PATH / \"noises\"\n",
    "    train_dir = DATA_PATH / \"hackaton_ds/train\"\n",
    "    train = ResnetTrainData(train_dir, noise_dir)\n",
    "    train_loader = DataLoader(train, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    train_answers = list()\n",
    "    for X, _ in tqdm(train_loader):\n",
    "        # X = X.unsqueeze(0)\n",
    "        preds = model.forward(X.to(DEVICE))\n",
    "        logists = softmax(preds[0]).cpu().data.numpy()\n",
    "        train_answers.append(logists)\n",
    "\n",
    "    return np.asfarray(train_answers), train.classes\n",
    "\n",
    "\n",
    "def efficientnet_train_infer(model):\n",
    "    noise_dir = PROJECT_DIR / \"hackaton_ds/noises\"\n",
    "    train_dir = PROJECT_DIR / \"hackaton_ds/train\"\n",
    "    train = EfficientnetTrainData(train_dir, noise_dir)\n",
    "    train_loader = DataLoader(train, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    train_answers = list()\n",
    "    for X, _ in tqdm(train_loader):\n",
    "        # X = X.unsqueeze(0)\n",
    "        preds = model.forward(X.to(DEVICE))\n",
    "        logists = softmax(preds[0]).cpu().data.numpy()\n",
    "        train_answers.append(logists)\n",
    "\n",
    "    return np.asfarray(train_answers), train.classes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test_dir = PROJECT_DIR / \"hackaton_ds/test\"\n",
    "    # test_markup = PROJECT_DIR / \"hackaton_ds/submission_xvector_cos_sim.csv\"\n",
    "    # test = TestData(test_dir, test_markup)\n",
    "    # test_loader = DataLoader(test, batch_size=1, shuffle=False, num_workers=0)\n",
    "    resnet = load_resnet_model().to(DEVICE)\n",
    "    resnet.load_state_dict(\n",
    "        torch.load(PROJECT_DIR / \"models/Resnet_16/resnet16_95ep.pt\")\n",
    "    )\n",
    "    resnet.eval()\n",
    "\n",
    "    efficientnet = load_efficientnet_model().to(DEVICE)\n",
    "    efficientnet.load_state_dict(\n",
    "        torch.load(PROJECT_DIR / \"models/Efficientnet/efficientnet_70ep.pt\")\n",
    "    )\n",
    "    efficientnet.eval()\n",
    "\n",
    "    resnet_train_answers, classes1 = resnet_train_infer(resnet)\n",
    "    classes = [CLASSES.index(class_name) for class_name in classes1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f54bea1301240e8a260b91e032dadd75d857f6331135606745e81825e79913d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
