{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoConfig, AudioClassificationPipeline\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTalTechNLP/voxlingua107-epaca-tdnn - 0.66\\nspeechbrain/google_speech_command_xvector - 0.90\\nanton-l/distilhubert-ft-common-language - 0\\nanton-l/sew-d-mid-400k-ft-keyword-spotting - >300 MB\\n\\n\\nw11wo/distil-wav2vec2-adult-child-cls-v3 - 199MB \\nanantoj/distil-wav2vec2-adult-child-cls - 144MB\\nntu-spml/distilhubert - 89MB\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TalTechNLP/voxlingua107-epaca-tdnn - 0.66\n",
    "speechbrain/google_speech_command_xvector - 0.90\n",
    "anton-l/distilhubert-ft-common-language - 0\n",
    "anton-l/sew-d-mid-400k-ft-keyword-spotting - >300 MB\n",
    "\n",
    "\n",
    "w11wo/distil-wav2vec2-adult-child-cls-v3 - 199MB \n",
    "anantoj/distil-wav2vec2-adult-child-cls - 144MB\n",
    "ntu-spml/distilhubert - 89MB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "WEIGHTS_PATH = Path(\"speechbrain/google_speech_command_xvector\")\n",
    "EXP_NAME = WEIGHTS_PATH.name\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "CLASSES = {\n",
    "    'yes': 0, \n",
    "    'no': 1, \n",
    "    'up': 2, \n",
    "    'down': 3, \n",
    "    'left': 4, \n",
    "    'right': 5, \n",
    "    'on': 6, \n",
    "    'off': 7, \n",
    "    'stop': 8, \n",
    "    'go': 9\n",
    "}\n",
    "INV_CLASSES = dict([[v, k] for k, v in CLASSES.items()])\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "N_NEIGHBORS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_classifier = EncoderClassifier.from_hparams(\n",
    "    source=WEIGHTS_PATH,\n",
    "    savedir=Path(\"pretrained_models\") / EXP_NAME,\n",
    "    run_opts={\"device\": DEVICE},\n",
    ")\n",
    "audio_normalizer = enc_classifier.audio_normalizer\n",
    "label_encoder = enc_classifier.hparams.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    signal, sr = torchaudio.load(str(path), channels_first=False)\n",
    "    return audio_normalizer(signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='cosine', n_jobs=-1, n_neighbors=12)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(\n",
    "    n_neighbors=N_NEIGHBORS,\n",
    "    metric=\"cosine\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be20feb9b83a497f9ca0b6c57f3ffc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_filepaths = sorted(list((DATA_PATH / \"train\").rglob(\"*.wav\")))\n",
    "rel_length = torch.tensor([1.0])\n",
    "lengths = []\n",
    "classes = []\n",
    "embeddings = []\n",
    "for audiofile in tqdm(audio_filepaths):\n",
    "    class_name = audiofile.parts[-2]\n",
    "\n",
    "    wav = load_audio(audiofile).unsqueeze(0)\n",
    "    embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0).squeeze(0)\n",
    "\n",
    "    classes.append(class_name)\n",
    "    embeddings.append(embedding.cpu().numpy())\n",
    "    lengths.append(wav.shape[-1] / 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.418"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEWDForSequenceClassification(\n",
      "  (sew_d): SEWDModel(\n",
      "    (feature_extractor): SEWDFeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): SEWDGroupNormConvLayer(\n",
      "          (conv): Conv1d(1, 64, kernel_size=(10,), stride=(5,), bias=False)\n",
      "          (layer_norm): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(64, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (2): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (3): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (4): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (5): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (6): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (7): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (8): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (9): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(256, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (10): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (11): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (12): SEWDNoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (feature_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): SEWDEncoder(\n",
      "      (pos_conv_embed): SEWDPositionalConvEmbedding(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(31,), stride=(2,), padding=(15,), groups=16)\n",
      "        (padding): SEWDSamePadLayer()\n",
      "      )\n",
      "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      (encoder): SEWDTransformerEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (1): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (2): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (3): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (4): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (5): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (6): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (7): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (8): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (9): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (10): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (11): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (12): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (13): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (14): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (15): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (16): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (17): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (18): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (19): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (20): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (21): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (22): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (23): SEWDLayer(\n",
      "            (attention): SEWDAttention(\n",
      "              (self): DisentangledSelfAttention(\n",
      "                (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (pos_dropout): StableDropout()\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "              (output): SEWDSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): SEWDIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            )\n",
      "            (output): SEWDOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (rel_embeddings): Embedding(512, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (upsample): SEWDUpsampling(\n",
      "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(WEIGHTS_PATH)\n",
    "# print(config)\n",
    "enc_model = AutoModelForAudioClassification.from_pretrained(WEIGHTS_PATH)\n",
    "enc_model.save_pretrained(Path(\"pretrained_models\") / EXP_NAME)\n",
    "enc_model.classifier = torch.nn.Linear(256, 256)\n",
    "enc_model = enc_model.to(DEVICE)\n",
    "print(enc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf176c1eb46c49ff96944af9494c4638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m wav, sr \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audiofile)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m wav \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m embedding \u001b[39m=\u001b[39m enc_model(wav)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m classes\u001b[39m.\u001b[39mappend(class_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/knn_embeddings.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m embeddings\u001b[39m.\u001b[39mappend(embedding\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:1685\u001b[0m, in \u001b[0;36mSEWDForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1681'>1682</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1682'>1683</a>\u001b[0m output_hidden_states \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum \u001b[39melse\u001b[39;00m output_hidden_states\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1684'>1685</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msew_d(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1685'>1686</a>\u001b[0m     input_values,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1686'>1687</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1687'>1688</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1688'>1689</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1689'>1690</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1690'>1691</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1692'>1693</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1693'>1694</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:1462\u001b[0m, in \u001b[0;36mSEWDModel.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1457'>1458</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_feature_vector_attention_mask(hidden_states\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], attention_mask)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1459'>1460</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_hidden_states(hidden_states, mask_time_indices\u001b[39m=\u001b[39mmask_time_indices)\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1461'>1462</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1462'>1463</a>\u001b[0m     hidden_states,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1463'>1464</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1464'>1465</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1465'>1466</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1466'>1467</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1467'>1468</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1469'>1470</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1471'>1472</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:1209\u001b[0m, in \u001b[0;36mSEWDEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1205'>1206</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m pooled_hidden_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :min_length] \u001b[39m+\u001b[39m position_embeddings[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :min_length]\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1206'>1207</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1208'>1209</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(hidden_states, attention_mask, output_hidden_states, output_attentions)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1210'>1211</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(encoder_outputs\u001b[39m.\u001b[39mlast_hidden_state)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1211'>1212</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m n_input_timesteps:\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:1128\u001b[0m, in \u001b[0;36mSEWDTransformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1118'>1119</a>\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1119'>1120</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1120'>1121</a>\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1124'>1125</a>\u001b[0m         rel_embeddings,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1125'>1126</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1126'>1127</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1127'>1128</a>\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1128'>1129</a>\u001b[0m         next_kv,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1129'>1130</a>\u001b[0m         attention_mask,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1130'>1131</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1131'>1132</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1132'>1133</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1133'>1134</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1134'>1135</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1136'>1137</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=1137'>1138</a>\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:970\u001b[0m, in \u001b[0;36mSEWDLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=960'>961</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=961'>962</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=962'>963</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=967'>968</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=968'>969</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=969'>970</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=970'>971</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=971'>972</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=972'>973</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=973'>974</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=974'>975</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=975'>976</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=976'>977</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=977'>978</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=978'>979</a>\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:901\u001b[0m, in \u001b[0;36mSEWDAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=891'>892</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=892'>893</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=893'>894</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=898'>899</a>\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=899'>900</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=900'>901</a>\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=901'>902</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=902'>903</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=903'>904</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=904'>905</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=905'>906</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=906'>907</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=907'>908</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=908'>909</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=909'>910</a>\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:773\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=767'>768</a>\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores\u001b[39m.\u001b[39mview(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=768'>769</a>\u001b[0m     \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, attention_scores\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), attention_scores\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=769'>770</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=771'>772</a>\u001b[0m \u001b[39m# bsz x height x length x dimension\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=772'>773</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m XSoftmax\u001b[39m.\u001b[39;49mapply(attention_scores, attention_mask, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=773'>774</a>\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=774'>775</a>\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=775'>776</a>\u001b[0m     attention_probs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attention_probs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), attention_probs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), value_layer\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=776'>777</a>\u001b[0m )\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py:522\u001b[0m, in \u001b[0;36mXSoftmax.forward\u001b[0;34m(self, input, mask, dim)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=518'>519</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=519'>520</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, mask, dim):\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=520'>521</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m dim\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=521'>522</a>\u001b[0m     rmask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39;49m(mask\u001b[39m.\u001b[39;49mbool())\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=523'>524</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mmasked_fill(rmask, \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/sew_d/modeling_sew_d.py?line=524'>525</a>\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_filepaths = sorted(list((DATA_PATH / \"train\").rglob(\"*.wav\")))\n",
    "\n",
    "classes = []\n",
    "embeddings = []\n",
    "for audiofile in tqdm(audio_filepaths):\n",
    "    class_name = audiofile.parts[-2]\n",
    "\n",
    "    wav, sr = torchaudio.load(audiofile)\n",
    "    wav = wav.to(DEVICE)\n",
    "    embedding = enc_model(wav).logits\n",
    "\n",
    "    classes.append(class_name)\n",
    "    embeddings.append(embedding.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"filepath\": [str(ap) for ap in audio_filepaths],\n",
    "        \"category\": [CLASSES[name] for name in classes],\n",
    "        \"embedding\": embeddings \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>category</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/train/down/0.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[15.127331, 11.903127, 3.8200674, -0.08791563,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/train/down/1.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[14.426889, 15.1516075, 3.523267, 2.2925537, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/train/down/10.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[17.777142, 2.83203, 0.29956368, 9.412637, 12....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/train/down/100.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[12.743387, 12.060911, 0.9379118, 7.697887, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/train/down/1000.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>[14.446474, 8.608712, 8.189811, 10.2905855, 8....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filepath  category  \\\n",
       "0     ../data/train/down/0.wav         3   \n",
       "1     ../data/train/down/1.wav         3   \n",
       "2    ../data/train/down/10.wav         3   \n",
       "3   ../data/train/down/100.wav         3   \n",
       "4  ../data/train/down/1000.wav         3   \n",
       "\n",
       "                                           embedding  \n",
       "0  [15.127331, 11.903127, 3.8200674, -0.08791563,...  \n",
       "1  [14.426889, 15.1516075, 3.523267, 2.2925537, 9...  \n",
       "2  [17.777142, 2.83203, 0.29956368, 9.412637, 12....  \n",
       "3  [12.743387, 12.060911, 0.9379118, 7.697887, 14...  \n",
       "4  [14.446474, 8.608712, 8.189811, 10.2905855, 8....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88790, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"../data/train_xvector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb535f5b72ad4b4181d1803aa3ebe4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeddings = torch.tensor(embeddings, device=DEVICE)\n",
    "\n",
    "pred = []\n",
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    # wav = load_audio(audiofile).unsqueeze(0)\n",
    "    # embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0)\n",
    "\n",
    "    wav, sr = torchaudio.load(audiofile)\n",
    "    wav = wav.to(DEVICE)\n",
    "    embedding = enc_model(wav).logits\n",
    "    \n",
    "    similarities = cos_sim(embedding, train_embeddings)\n",
    "    max_similar_idx = similarities.argmax()\n",
    "    class_name = classes[max_similar_idx]\n",
    "\n",
    "    pred.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "down    29620\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "down     3208\n",
       "stop     2967\n",
       "left     2965\n",
       "on       2954\n",
       "up       2950\n",
       "off      2932\n",
       "yes      2932\n",
       "no       2924\n",
       "right    2916\n",
       "go       2872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data={\n",
    "    \"id\": [a.stem for a in test_audio_filepaths],\n",
    "    \"category\": pred,\n",
    "})\n",
    "sub.to_csv(f\"submission_{EXP_NAME}_cos_sim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN\n",
    "classifier.fit(embeddings, [CLASSES[name] for name in classes])\n",
    "# save knn model\n",
    "with open(\"knn_xvector.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(classifier, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6dcaad932649499f8b7354be0717b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_audio_filepaths = sorted(list((DATA_PATH / \"test\").glob(\"*.wav\")))\n",
    "test_embeddings = []\n",
    "for audiofile in tqdm(test_audio_filepaths):\n",
    "    wav = load_audio(audiofile).unsqueeze(0)\n",
    "    embedding = enc_classifier.encode_batch(wav, rel_length).squeeze(0).squeeze(0)\n",
    "    test_embeddings.append(embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [INV_CLASSES[idx] for idx in classifier.predict(test_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data={\n",
    "    \"id\": [a.stem for a in test_audio_filepaths],\n",
    "    \"category\": pred,\n",
    "})\n",
    "sub.to_csv(f\"submission_{EXP_NAME}_knn12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f54bea1301240e8a260b91e032dadd75d857f6331135606745e81825e79913d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
