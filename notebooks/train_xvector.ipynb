{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_metric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "DATA_PATH = Path(\"../data\")\n",
    "WEIGHTS_PATH = Path(\"speechbrain/google_speech_command_xvector\")\n",
    "EXP_NAME = WEIGHTS_PATH.name\n",
    "\n",
    "MAX_AUDIO_LEN = 16000  # в отсчётах sr\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 100\n",
    "VAL_ITER = 10\n",
    "CLASSES = [\n",
    "    \"down\",\n",
    "    \"go\",\n",
    "    \"left\",\n",
    "    \"no\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"right\",\n",
    "    \"stop\",\n",
    "    \"up\",\n",
    "    \"yes\",\n",
    "]\n",
    "label2id = dict(dict([[k, v] for k, v in enumerate(CLASSES)]))\n",
    "id2label = dict(dict([[v, k] for k, v in enumerate(CLASSES)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_classifier = EncoderClassifier.from_hparams(\n",
    "    source=WEIGHTS_PATH,\n",
    "    savedir=Path(\"pretrained_models\") / EXP_NAME,\n",
    "    run_opts={\"device\": DEVICE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['compute_features', 'mean_var_norm', 'embedding_model', 'classifier'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_classifier.mods.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_normalizer = enc_classifier.audio_normalizer\n",
    "feature_extractor1 = enc_classifier.mods.compute_features\n",
    "normalizer = enc_classifier.mods.mean_var_norm\n",
    "embedding_model = enc_classifier.mods.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, audio_filepaths: list, noise_dir: Path, aug=False) -> None:\n",
    "        super().__init__()\n",
    "        self.audio_len = MAX_AUDIO_LEN\n",
    "        self.aug = aug\n",
    "\n",
    "        self.audios = list()\n",
    "        self.noises = list()\n",
    "        self.classes = list()\n",
    "\n",
    "        for wav_path in audio_filepaths:\n",
    "            audio, _ = torchaudio.load(wav_path)\n",
    "            self.audios.append(audio)\n",
    "            self.classes.append(wav_path.parts[-2])\n",
    "\n",
    "        if aug:\n",
    "            for wav_path in noise_dir.iterdir():\n",
    "                noise, _ = torchaudio.load(wav_path)\n",
    "                self.noises.append(noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.aug:\n",
    "            noise_idx = np.random.randint(0, len(self.noises))\n",
    "            noise = self.noises[noise_idx]\n",
    "\n",
    "            if noise.shape[0] == 2:\n",
    "                noise = noise[np.random.randint(0, 2)] # TODO: try mean instead random\n",
    "                noise = noise.unsqueeze(0)\n",
    "\n",
    "        audio = self.audios[idx]\n",
    "        audio = self.__pad_audio(audio)\n",
    "        if self.aug:\n",
    "            audio = self.__add_noise(audio, noise, 0, 6)\n",
    "        audio = audio / audio.abs().max()\n",
    "\n",
    "        return audio, CLASSES.index(self.classes[idx])\n",
    "\n",
    "    def __pad_audio(self, audio):\n",
    "        if self.audio_len - audio.shape[-1] > 0:\n",
    "            i = np.random.randint(0, self.audio_len - audio.shape[-1])\n",
    "        else:\n",
    "            i = 0\n",
    "        pad_patern = (i, self.audio_len - audio.shape[-1] - i)\n",
    "        audio = F.pad(audio, pad_patern, \"constant\").detach()\n",
    "        return audio\n",
    "\n",
    "    def __add_noise(self, clean, noise, min_amp, max_amp):\n",
    "        noise_amp = np.random.uniform(min_amp, max_amp)\n",
    "        # так как шумная запись длиннее, то выбираем случайный момент начала шумной записи\n",
    "        start = np.random.randint(0, noise.shape[1] - clean.shape[1] + 1)\n",
    "        noise_part = noise[:, start : start + clean.shape[1]]\n",
    "\n",
    "        if noise_part.abs().max() == 1:\n",
    "            return clean\n",
    "\n",
    "        # накладываем шум\n",
    "        noise_mult = clean.abs().max() / noise_part.abs().max() * noise_amp\n",
    "        return (clean + noise_part * noise_mult) / (1 + noise_amp)\n",
    "\n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, audio_filepaths: list) -> None:\n",
    "        super().__init__()\n",
    "        self.audio_len = MAX_AUDIO_LEN\n",
    "\n",
    "        self.audios = list()\n",
    "        for file_name in audio_filepaths:\n",
    "            audio, _ = torchaudio.load(file_name)\n",
    "            self.audios.append(audio)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.audios[idx]\n",
    "        audio = self.__pad_audio(audio)\n",
    "        audio = audio / audio.abs().max()\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def __pad_audio(self, audio):\n",
    "        if self.audio_len - audio.shape[-1] > 0:\n",
    "            i = np.random.randint(0, self.audio_len - audio.shape[-1])\n",
    "        else:\n",
    "            i = 0\n",
    "        pad_patern = (i, self.audio_len - audio.shape[-1] - i)\n",
    "        audio = F.pad(audio, pad_patern, \"constant\").detach()\n",
    "\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "audio_filpaths = sorted(list((DATA_PATH / \"train\").rglob(\"*.wav\")))\n",
    "classes = [p.parts[-2] for p in audio_filpaths]\n",
    "train_audio_paths, val_audio_paths, train_classes, val_classes = \\\n",
    "    train_test_split(audio_filpaths, classes, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainData(train_audio_paths, noise_dir=DATA_PATH / \"noises\", aug=True)\n",
    "val_dataset = TrainData(val_audio_paths, noise_dir=DATA_PATH / \"noises\", aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71032, 17758)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/speechbrain/google_speech_command_xvector/resolve/main/config.json from cache at /home/and/.cache/huggingface/transformers/189eff8c2b250ceb71be715e2335959234fc8bdbb1b504c508dda42f3d6ea844.b7fc0832e158a106fdd233d8ba1e7b54eb928fdb08345ba2e6510001573ad668\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in speechbrain/google_speech_command_xvector. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: yoso, swin, vilt, vit_mae, realm, nystromformer, imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForAudioClassification, TrainingArguments, Trainer\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForAudioClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m     WEIGHTS_PATH, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m     num_labels\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(CLASSES),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=5'>6</a>\u001b[0m     label2id\u001b[39m=\u001b[39;49mlabel2id,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=6'>7</a>\u001b[0m     id2label\u001b[39m=\u001b[39;49mid2label,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000013vscode-remote?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:424\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=421'>422</a>\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_auto\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=422'>423</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=423'>424</a>\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=424'>425</a>\u001b[0m         pretrained_model_name_or_path, return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=425'>426</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=426'>427</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=427'>428</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:640\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=636'>637</a>\u001b[0m         \u001b[39mif\u001b[39;00m pattern \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=637'>638</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m config_class\u001b[39m.\u001b[39mfrom_dict(config_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=639'>640</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=640'>641</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized model in \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=641'>642</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould have a `model_type` key in its \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m, or contain one of the following strings \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=642'>643</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min its name: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(CONFIG_MAPPING\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py?line=643'>644</a>\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in speechbrain/google_speech_command_xvector. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: yoso, swin, vilt, vit_mae, realm, nystromformer, imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    WEIGHTS_PATH, \n",
    "    num_labels=len(CLASSES),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "# model = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{EXP_NAME}-finetuned-ks\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c59584864343f59cfff598f8e31063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/and/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 71032\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.33/home/and/projects/hacks/tv-neuro-tech-speech-rec/notebooks/train_xvector.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py:1339\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1335'>1336</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1337'>1338</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1338'>1339</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1339'>1340</a>\u001b[0m \n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1340'>1341</a>\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1341'>1342</a>\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/trainer.py?line=1342'>1343</a>\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=59'>60</a>\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=60'>61</a>\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=61'>62</a>\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=62'>63</a>\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=66'>67</a>\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py:105\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=101'>102</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(features[\u001b[39m0\u001b[39m], (\u001b[39mdict\u001b[39m, BatchEncoding)):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=104'>105</a>\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mvars\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=105'>106</a>\u001b[0m first \u001b[39m=\u001b[39m features[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=106'>107</a>\u001b[0m batch \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py:105\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=101'>102</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(features[\u001b[39m0\u001b[39m], (\u001b[39mdict\u001b[39m, BatchEncoding)):\n\u001b[0;32m--> <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=104'>105</a>\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mvars\u001b[39;49m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=105'>106</a>\u001b[0m first \u001b[39m=\u001b[39m features[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/projects/hacks/tv-neuro-tech-speech-rec/.venv/lib/python3.8/site-packages/transformers/data/data_collator.py?line=106'>107</a>\u001b[0m batch \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f54bea1301240e8a260b91e032dadd75d857f6331135606745e81825e79913d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
